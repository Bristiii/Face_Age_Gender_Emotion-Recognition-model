{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6db7b02-45c0-4509-bad1-d0800df5df61",
   "metadata": {},
   "source": [
    "## The project for Face,Age,gender,emotion Recognition is starting from now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b16fad0-23b3-4c2a-ae2d-9dd9a19fa1ed",
   "metadata": {},
   "source": [
    "###### just in my virtual environment i installed the opencv-contrib-python in oredr to make this project, simply activated the venv by opening the folder from my dekstop where exactly i created the virtual env and by entering the promt \"Bristi-virtual-env\\Scripts\\activate\" i activated the env and using \"pip install opencv-contrib-python\" i installed the opencv package,where opencv-python\tMain OpenCV modules (basic functionality)\n",
    "###### opencv-contrib-python\tMain modules + extra modules from the \"contrib\" repository "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fb38ae-95d2-4108-aa3f-cecf60ab934f",
   "metadata": {},
   "source": [
    "##### now for downloading the dataset go in google search for\"haarcascade frontal face xml default \" and open the kaggale and by right clicking and saving it it will be downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef82f0a-382d-47cf-aaa5-0fb3923a67e5",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52f237a3-9ed1-4f85-a434-4662e7282931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # for reading and writing in files\n",
    "import cv2 # for detecting and recognizing the face in web camera\n",
    "import numpy as np #for numerical computation\n",
    "from deepface import DeepFace #DeepFace is a deep learning model used for face recognition and verification by converting faces into numerical embeddings for accurate comparison. It enables tasks like identity matching, emotion, age, and gender detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6135b2bf-4549-4e70-89ef-3fb6eedfba79",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be721e08-9dbe-4f49-9cb8-31edb316f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"face_dataset\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "def create_face_dataset(name):\n",
    "    person_dir = os.path.join(data_dir, name)  # for each person there images will be stored in a directory\n",
    "    os.makedirs(person_dir, exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    print(\"capturing Images.Press'q' to quit\")# opencv will capture the video and 0 is for webcam\n",
    "    count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Cannot Capture Image\")\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # convert to grayscale for detection\n",
    "        faces = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\\\n",
    "                 .detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            count += 1\n",
    "            face_img = frame[y:y+h, x:x+w]\n",
    "            face_path = os.path.join(person_dir, f\"{name}_{count}.jpg\")\n",
    "            cv2.imwrite(face_path, face_img)\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        cv2.imshow(\"Capture Face In Camera\", frame)\n",
    "\n",
    "            \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q') or count >= 50:\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"Saved{count}images for {name}.\")\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ce4805-393c-44b2-8dc4-4dd36219ba9d",
   "metadata": {},
   "source": [
    "## Train the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af654ed7-42a8-4e84-be03-ce0def27e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_face_dataset():\n",
    "    embeddings = {}  # extract all the facial feature and convert it into the numerical values\n",
    "    for person in os.listdir(data_dir):\n",
    "        person_dir = os.path.join(data_dir, person)\n",
    "        if os.path.isdir(person_dir):\n",
    "            embeddings[person] = []\n",
    "            for img_name in os.listdir(person_dir):\n",
    "                img_path = os.path.join(person_dir, img_name)\n",
    "                try:\n",
    "                    embedding = DeepFace.represent(img_path, model_name=\"Facenet\", enforce_detection=False)[0][\"embedding\"]\n",
    "                    embeddings[person].append(embedding)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to train image: {img_path}:{e}\")\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fcac58-bc13-47d4-a2f3-20562247aa98",
   "metadata": {},
   "source": [
    "## Recognize the Face,age,gender,emotion using Deepface (Facenet model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69904e74-aef1-450e-a867-6bb01d29052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_faces(embeddings):\n",
    "    cap = cv2.VideoCapture(0)  \n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"failed to capture Image\")\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  \n",
    "\n",
    "        faces = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\\\n",
    "            .detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_img = frame[y:y+h, x:x+w]\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "            try:\n",
    "                analyse = DeepFace.analyze(face_img, actions=[\"age\", \"gender\", \"emotion\"], enforce_detection=False)\n",
    "                if isinstance(analyse, list):\n",
    "                    analyse = analyse[0]\n",
    "\n",
    "                age = analyse[\"age\"]\n",
    "\n",
    "                gender = analyse[\"dominant_gender\"]  \n",
    "\n",
    "                gender =gender if isinstance(gender,str)else max(gender, key=gender.get)\n",
    "\n",
    "                emotion = max(analyse[\"emotion\"], key=analyse[\"emotion\"].get)\n",
    "                #recognize person\n",
    "                face_embedding = DeepFace.represent(face_img, model_name=\"Facenet\", enforce_detection=False)[0][\"embedding\"]\n",
    "                #calculate cosine similarity\n",
    "                match = None\n",
    "                max_similarity = -1  \n",
    "\n",
    "                for person, person_embeddings in embeddings.items():  \n",
    "                    for embed in person_embeddings:\n",
    "                        similarity = np.dot(face_embedding, embed) / (np.linalg.norm(face_embedding) * np.linalg.norm(embed))\n",
    "                        if similarity > max_similarity:\n",
    "                            max_similarity = similarity\n",
    "                            match = person\n",
    "\n",
    "                if max_similarity > 0.7:\n",
    "                    label = f\"{match} ({max_similarity:.2f})\" \n",
    "                    #print(f\"Recognized{label}--Exiting recognition.\")\n",
    "                    #cap.release()\n",
    "                    #cv2.destroyAllWindows()\n",
    "                else:\n",
    "                    label = \"Unknown person\"\n",
    "\n",
    "                display_text = f\"{label}, Age: {int(age)}, Gender: {gender}, Emotion: {emotion}\"\n",
    "                cv2.putText(frame, display_text, org=(x, y - 10),\n",
    "                            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5,\n",
    "                            color=(255, 255, 255), thickness=2)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"CAN NOT RECOGNIZE FACES\")\n",
    "\n",
    "        cv2.imshow(\"Recognize Faces\", frame)  \n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"Pressed Q.Exiting\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365f3b24-cb16-458c-a6a7-f3b44e4a6e8d",
   "metadata": {},
   "source": [
    "## OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71f6b2ca-ab68-4cf8-8e2c-e24af5ee4faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Create face Dataset\n",
      "2.Train Face Dataset\n",
      "3.Recognize faces\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice:  1\n",
      "Enter name of the person:  Tinku Ghosh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capturing Images.Press'q' to quit\n",
      "Saved1images for Tinku Ghosh.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"1.Create face Dataset\\n2.Train Face Dataset\\n3.Recognize faces\")\n",
    "    choice = input(\"Enter your choice: \")\n",
    "\n",
    "    if choice == \"1\":\n",
    "        name = input(\"Enter name of the person: \")\n",
    "        create_face_dataset(name)\n",
    "\n",
    "    elif choice == \"2\":\n",
    "        embeddings = train_face_dataset()\n",
    "        np.save(\"embeddings.npy\", embeddings)\n",
    "        print(\"Embedding Saved\")\n",
    "\n",
    "    elif choice == \"3\":\n",
    "        if os.path.exists(\"embeddings.npy\"):\n",
    "            embeddings = np.load(\"embeddings.npy\", allow_pickle=True).item()\n",
    "            recognize_faces(embeddings)\n",
    "        else:\n",
    "            print(\"No File is Found\")\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid choice\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fdac31c4-0f1d-4f95-8b40-7c870c13c703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a0a9d25-fead-4a94-9b0a-ed9bacc4b81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bristi Ghosh']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"Dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7100cd5-b0fb-4e4a-b436-624f04db20f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bristi Ghosh_1.jpg',\n",
       " 'Bristi Ghosh_10.jpg',\n",
       " 'Bristi Ghosh_11.jpg',\n",
       " 'Bristi Ghosh_12.jpg',\n",
       " 'Bristi Ghosh_13.jpg',\n",
       " 'Bristi Ghosh_14.jpg',\n",
       " 'Bristi Ghosh_15.jpg',\n",
       " 'Bristi Ghosh_17.jpg',\n",
       " 'Bristi Ghosh_18.jpg',\n",
       " 'Bristi Ghosh_19.jpg',\n",
       " 'Bristi Ghosh_2.jpg',\n",
       " 'Bristi Ghosh_20.jpg',\n",
       " 'Bristi Ghosh_21.jpg',\n",
       " 'Bristi Ghosh_22.jpg',\n",
       " 'Bristi Ghosh_23.jpg',\n",
       " 'Bristi Ghosh_24.jpg',\n",
       " 'Bristi Ghosh_25.jpg',\n",
       " 'Bristi Ghosh_26.jpg',\n",
       " 'Bristi Ghosh_27.jpg',\n",
       " 'Bristi Ghosh_28.jpg',\n",
       " 'Bristi Ghosh_29.jpg',\n",
       " 'Bristi Ghosh_3.jpg',\n",
       " 'Bristi Ghosh_30.jpg',\n",
       " 'Bristi Ghosh_31.jpg',\n",
       " 'Bristi Ghosh_32.jpg',\n",
       " 'Bristi Ghosh_33.jpg',\n",
       " 'Bristi Ghosh_34.jpg',\n",
       " 'Bristi Ghosh_35.jpg',\n",
       " 'Bristi Ghosh_36.jpg',\n",
       " 'Bristi Ghosh_37.jpg',\n",
       " 'Bristi Ghosh_38.jpg',\n",
       " 'Bristi Ghosh_39.jpg',\n",
       " 'Bristi Ghosh_4.jpg',\n",
       " 'Bristi Ghosh_40.jpg',\n",
       " 'Bristi Ghosh_41.jpg',\n",
       " 'Bristi Ghosh_42.jpg',\n",
       " 'Bristi Ghosh_43.jpg',\n",
       " 'Bristi Ghosh_44.jpg',\n",
       " 'Bristi Ghosh_45.jpg',\n",
       " 'Bristi Ghosh_46.jpg',\n",
       " 'Bristi Ghosh_47.jpg',\n",
       " 'Bristi Ghosh_48.jpg',\n",
       " 'Bristi Ghosh_49.jpg',\n",
       " 'Bristi Ghosh_5.jpg',\n",
       " 'Bristi Ghosh_50.jpg',\n",
       " 'Bristi Ghosh_6.jpg',\n",
       " 'Bristi Ghosh_7.jpg',\n",
       " 'Bristi Ghosh_8.jpg']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"Dataset/Bristi Ghosh\")  # replace with actual name\n",
    "#This PC > Local Disk (C:) > Users > DELL > Dataset\n",
    "#this is to find where the dataset is being stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "317ace35-9332-4e7b-bd90-ce28c9637107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.startfile(\"Dataset\")  # will open the folder in File Explorer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a536827d-7e93-4cac-94af-17405d5517bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyNewVenv",
   "language": "python",
   "name": "mynewvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
